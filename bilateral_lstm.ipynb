{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Tensorflow packages ------------------ #\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization  # Replaces Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences  # Moved location\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, LSTM, Embedding, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# -------- other packages ------------------ #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import pickle\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)  # Show full numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"keywords_output.csv\", names = [\"paragraphs\", \"label\"])\n",
    "df['paragraphs'] = df['paragraphs'].astype(str)\n",
    "df['label'] = df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(\n",
    "    max_tokens=100000,  # Set the maximum vocabulary size\n",
    "    output_mode=\"int\",  # Output integer-encoded sequences\n",
    "    output_sequence_length=512,  # Set the desired sequence length\n",
    "    standardize=\"lower_and_strip_punctuation\",  # Optional: text preprocessing\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the sentences\n",
    "vectorizer.adapt(df[\"paragraphs\"])\n",
    "\n",
    "# Transform sentences into integer-encoded sequences using the 'call' method\n",
    "encoded_sequences = vectorizer(df[\"paragraphs\"]).numpy()\n",
    "\n",
    "# Initialize empty lists for sentence and keyword columns\n",
    "sentence_column = []\n",
    "keyword_column = []\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    new_keywords = []\n",
    "    sentence = row[\"paragraphs\"]\n",
    "    keywords = row[\"label\"]\n",
    "    tokens = sentence.split()  # Split sentence into tokens (words)\n",
    "    for token in tokens:\n",
    "        if token in keywords:\n",
    "            if not any(char.isdigit() for char in token):\n",
    "                new_keywords.append(1)\n",
    "        else:\n",
    "            new_keywords.append(0)\n",
    "    if sum(new_keywords) != 0:\n",
    "        sentence_column.append(sentence)\n",
    "        keyword_column.append(new_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer.adapt(sentence_column)\n",
    "X = vectorizer(sentence_column).numpy()\n",
    "X = pad_sequences(X, padding = \"post\", truncating = \"post\", maxlen = 512, value = 0)\n",
    "y = pad_sequences(keyword_column, padding = \"post\", truncating = \"post\", maxlen = 512, value = 0)\n",
    "y = [to_categorical(i, num_classes = 2) for i in y]\n",
    "embeddings_index = {}\n",
    "f = open('embeddings.txt','r')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype = \"float32\")\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_index = vectorizer.get_vocabulary()\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "ed = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, ed))\n",
    "\n",
    "# Populate the embedding matrix\n",
    "for word, i in enumerate(word_index):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights = [embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True, recurrent_dropout = 0.1)))\n",
    "model.add(TimeDistributed(Dense(2, activation = \"softmax\")))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.fit(X_train, np.array(y_train), batch_size = 10, epochs = 1, validation_split = 0.1)\n",
    "model_json = model.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"If n is a billion then this is about 12000 right The denominators keep getting bigger and bigger but the numerators stay the same theyre always x So when I take the product if I go far enough out Im going to be multiplying by very very small numbers and more and more of them And so no matter what x is these numbers will converge to 0 Theyll get smaller and smaller as x gets to be bigger Thats the sign that x is inside of the radius of convergence This is the sign for you that this series converges for that value of x And because I could do this for any x this works This convergence to 0 for any fixed x Thats what tells you that you can take that the radius of convergence is infinity Because in the formula in the fact in this property that the radius of convergence talks about if R is equal to infinity this is no condition on x Every number is less than infinity in absolute value So if this convergence to 0 of the general term works for every x then radius of convergence is infinity Well that was kind of fast but I think that youve heard something about that earlier as well Anyway so weve got the sine function a new function with its own power series Its a way of computing sinx If you take enough terms youll get a good evaluation of sinx for any x This tells you a lot about the function sinx but not everything at all For example from this formula its very hard to see that the sine of x is periodic Its not obvious at all Somewhere hidden away in this expression is the number pi the half of the period But thats not clear from the power series at all So the power series are very good for some things but they hide other properties of functions Well so I want to spend a few minutes telling you about what you can do with a power series once you have one to get new power series so new power series from old And this is also called operations on power series So what are the things that we can do to a power series Well one of the things you can do is multiply So for example what if I want to compute a power series for x sinx Well I have a power series for sinx I just did it How about a power series for x Actually I did that here too The function x is a very simple polynomial Its a polynomial where thats 0 a_1 is 1 and all the other coefficients are 0 So x itself is a power series a very simple one sinx is a powers series And what I want to encourage you to do is treat power series just like polynomials and multiply them together Well see other operations too So to compute the power series for x sinx of I just take this one and multiply it by x So lets see if I can do that right\"\n",
    "\n",
    "vectorizer.adapt(sentence_column)\n",
    "encoded_example = vectorizer(sentence_column).numpy()\n",
    "padded_example = pad_sequences(encoded_example, maxlen=512, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = model.predict(padded_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming binary classification (1 for relevant keyword, 0 for not relevant)\n",
    "predicted_keywords = [1 if pred[0] >= 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Print the predicted keywords\n",
    "print(\"Predicted keywords for the example sentence:\")\n",
    "print(predicted_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m(predictions))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(pd.shape(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.weights.h5\")\n",
    "with open(\"vectorizer.pickle\", \"rb\") as vectorizer_file:\n",
    "    vectorizer = pickle.load(vectorizer_file)\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load the saved model architecture from JSON file\n",
    "with open(\"model.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the saved model weights\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(\"tokenizer.pickle\", \"rb\") as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# Example sentence to test the model\n",
    "example_sentence = \"This is an example sentence about natural language processing.\"\n",
    "\n",
    "# Preprocess the example sentence (tokenize and pad)\n",
    "encoded_example = tokenizer.texts_to_sequences([example_sentence])\n",
    "padded_example = tf.keras.preprocessing.sequence.pad_sequences(encoded_example, maxlen=512, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(padded_example)\n",
    "\n",
    "# Assuming binary classification (1 for relevant keyword, 0 for not relevant)\n",
    "predicted_keywords = [1 if pred[0] >= 0.5 else 0 for pred in predictions]\n",
    "\n",
    "# Print the predicted keywords\n",
    "print(\"Predicted keywords for the example sentence:\")\n",
    "print(predicted_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
