{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from transformers import BertTokenizer\n",
    "# import torch\n",
    "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('output.csv')\n",
    "\n",
    "# # Assume 'input' is the column with text, and 'key' is the column with labels\n",
    "# texts = df['input'].tolist()\n",
    "# labels = df['key'].tolist()\n",
    "# print(\"1\")\n",
    "# # Encode the labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "#     texts, encoded_labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "# print(\"2\")\n",
    "# # Initialize the BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize and encode texts and labels for BERT\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "# test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "# print(\"3\")\n",
    "# class KeywordDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "# print(\"3\")\n",
    "# # Create the dataset\n",
    "# train_dataset = KeywordDataset(train_encodings, train_labels)\n",
    "# test_dataset = KeywordDataset(test_encodings, test_labels)\n",
    "\n",
    "# # Load the pre-trained BERT model\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=50,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "# print(\"4\")\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset\n",
    "# )\n",
    "# print(\"5\")\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "# print(\"6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\stuff\\college\\ai assignment\\bert_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('1')\n",
    "# Load the dataset\n",
    "df = pd.read_csv('keywords_output.csv')\n",
    "\n",
    "# Assume 'input' is the column with text, and 'key' is the column with labels\n",
    "texts = df['paragraphs'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, encoded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print('2')\n",
    "# Initialize the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize and encode texts and labels for DistilBert\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "print('3')\n",
    "class KeywordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = KeywordDataset(train_encodings, train_labels)\n",
    "test_dataset = KeywordDataset(test_encodings, test_labels)\n",
    "print(\"4\")\n",
    "# Load the pre-trained DistilBert model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=30,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "print('5')\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print('6')\n",
    "# Save the trained model and tokenizer\n",
    "model_path = r\"D:\\stuff\\college\\ai assignment\\bert_model\"\n",
    "tokenizer_path = r\"D:\\stuff\\college\\ai assignment\\bert_model_token\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "print(f\"Model and tokenizer have been saved successfully to {model_path} and {tokenizer_path} respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2725\n",
      "Predicted keywords: [\"['expectation', 'convex', 'jensens', 'expect', 'bar']\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from the saved configuration\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(r'D:\\stuff\\college\\ai assignment\\bert_model_token')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(r'D:\\stuff\\college\\ai assignment\\bert_model')\n",
    "\n",
    "# Prepare the sentence\n",
    "sentence = \"Yeah question AUDIENCE INAUDIBLE PROFESSOR How do you make sure that x and y are inits or floats So this is something that you could write in the specifications so the docstring with the triple quotes So whoever uses the class would then know that if they do something outside the specification the code might not work as expected Or you could put in a cert statement inside the definition of the init just to sort of force that Force that to be true Great question Yeah question AUDIENCE INAUDIBLE PROFESSOR Does the x does this self x and this x have to be the same name The answer is no And were going to see in class exercise that you can have it be different OK Great So this defines the way that we create an object So now we have sort of a nice class Its very simple but we can start actually creating coordinate objects So when you create coordinate objects youre creating instances of the class So this line here C is equal to coordinate 34 is going to call the init method Its going to call the init method with x is equal to 3 and y is equal to 4 Im just going to go over here and I wrote this previously because notice when were creating an object here were only giving it two parameters But in the init method we have actually three parameters right We have these three parameters here but when were creating an object we only give it two parameters And thats OK because implicitly Python is going to say self is going to be this object C so just by default OK So when youre creating a coordinate object youre passing it all the variables except for self So this line here is going to call the init and its going to do every line inside the init So its going to create an x data attribute for C a y data attribute for C and its going to assign 3 and 4 to those respectively This next line here is origin equals coordinate 0 0 creates another object OK Its another coordinate object whose value for x is 0 and whose value for y is 0 So now we have two coordinate objects We can access the data attributes using this dot notation and weve seen that before right When weve worked with lists wed say something like L dot append right when we create a list So the same dot notation can be used with your own objects in order to access data attributes So here this is going to print 3 because the x value for object C is 3 and the next line print origin x is going to print 0 because the x value for the object origin is 0 OK So weve created a coordinate object We have to find the init method so we have a way to create objects when we use the class And then we can access the data attributes But thats kind of lame right because there isnt anything cool we can do with it There isnt ways to interact with this object\"\n",
    "# Tokenize and encode the sentence for DistilBert\n",
    "print(len(sentence))\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get predicted keyword indi\n",
    "predicted_keyword_indices = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Convert indices to keywords using the label encoder\n",
    "keywords = [label_encoder.classes_[index] for index in predicted_keyword_indices.numpy()]\n",
    "\n",
    "print(f\"Predicted keywords: {keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(inputs)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get predicted keyword indices\u001b[39;00m\n\u001b[0;32m     15\u001b[0m predicted_keyword_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\stuff\\college\\ai assignment\\bert_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DistilBertForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from the saved configuration\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(r'D:\\stuff\\college\\ai assignment\\bert_model_token')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(r'D:\\stuff\\college\\ai assignment\\bert_model')\n",
    "\n",
    "# Prepare the sentence\n",
    "sentence = \"So Poisson the likelihood in X1 Xn and lambda was equal to lambda to the sum of the xis e to the minus n lambda divided by X1 factorial all the way to Xn factorial So let me take the log likelihood Thats going to be equal to what Its going to tell me Its going to be well let me get rid of this guy first Minus log of X1 factorial all the way to Xn factorial Thats a constant with respect to lambda So when Im going to take the derivative its going to go Then Im going to have plus sum of the xis times log lambda And then Im going to have minus n lambda So now then you take the derivative and set it equal to zero So log L well partial with respect to lambda of log L say lambda equals zero This is equivalent to so this guy goes This guy gives me sum of the xis divided by lambda hat equals n And so thats equivalent to lambda hat is equal to sum of the xis divided by n which is Xn bar Take derivative set it equal to zero and just solve Its a very satisfying exercise especially when you get the average in the end You dont have to think about it forever OK the Gaussian model Im going to leave to you as an exercise Take the log to get rid of the pesky exponential and then take the derivative and you should be fine Its a bit more it might be one more line than those guys OK so well actually you need to take the gradient in this case Dont check the second derivative right now You dont have to really think about it What did I want to add I think there was something I wanted to say Yes When I have a function thats concave and Im on like some infinite interval then its true that taking the derivative and setting it equal to zero will give me the maximum But again I might have a function that looks like this Now if Im on some finite interval let me go elsewhere So if Im on some finite interval and my function looks like this as a function of theta lets say this is my log likelihood as a function of theta then OK theres no place in this interval lets say this is between 0 and 1 theres no place in this interval where the derivative is equal to 0 And if you actually try to solve this you wont find a solution which is not in the interval 0 1 And thats actually how you know that you probably should not take the derivative equal to zero So dont panic if you get something that says well the solution is at infinity right If this function keeps going you will find that the solution you wont be able to find a solution apart from infinity You are going to see something like 1 over theta hat is equal to 0 or something like this So you know that when youve found this kind of solution youve probably made a mistake at some point And the reason is because the functions that are like this you dont find the maximum by setting the derivative equal to zero You actually just find the maximum by saying well its an increasing function on the interval 0 1 so the maximum must be attained at 1 So here in this case that would mean that my maximum would be 1\"\n",
    "\n",
    "# Tokenize and encode the sentence for DistilBert\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get predicted keyword indices\n",
    "predicted_keyword_indices = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Convert indices to keywords using the label encoder\n",
    "keywords = [label_encoder.classes_[index] for index in predicted_keyword_indices.numpy()]\n",
    "\n",
    "print(f\"Predicted keywords: {keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"PROFESSOR Hi and welcome Today were going to do a problem about powers of a matrix Our problem is first to find a formula for the kth power of this matrix C This is a two by two matrix that depends on variables a and b And the second part of our problem is to calculate C to the 100th in the special case where a and b are 1 You can hit pause now and Ill give you a minute to do the problem yourself And then Ill come back and we can do it together OK Were back Now whats the first step in finding powers of a matrix Well we need to find the eigenvalues and eigenvectors of this matrix So how do we do that We compute the determinant of C minus lambda I which is just the determinant of this matrix 2b minus a minus lambda 2b minus 2a a minus b and 2a minus b minus lambda OK If you compute this well we have a lambda squared term OK Our lambda term if you look at it youll see we get 2b minus a plus 2a minus b which is just a plus b And we have a negative sign And its negative a plus b times lambda And our last term is a little tougher to compute So Ill let you do it yourself But youre just going to get plus ab And this will factor as lambda minus a times lambda minus b So our eigenvalues are just a and b Now we need to find our eigenvectors So how do we do that Well what we need to do is we need to look at C minus a times the identity And we need to find the null space of this matrix So what do we get here We get 2b minus 2a And then our next entry here we get 2a minus b minus a So this is a minus b Good So you can see that this matrix has the same columns and the same rows And so you can see that a vector in the null space since this column is 2 times this column we can see that our first eigenvector is just or 1 2 I should say Its just 1 2 Good Well I guess we have space to do the second one too Why not So lets write out the second one also Here were subtracting b instead of a You get b minus a You get 2b minus 2a We get a minus b And what do we have here We have 2a minus 2b So now whats in the null space of this matrix Well what you can see is that this column is 1 times that column So our second eigenvector is just going to be 1 1 And I should remind you that if you have a harder example you can just find these null spaces by elimination like we always do Great Now we have our eigenvalues and our eigenvectors So now we can write C in a nice easy way that allows us to take powers of it So whats that way So thats C equals S lambda S inverse So this is just what is S Remember S is our matrix of eigenvectors So S is the matrix 1 2 1 1 Good Now what is lambda Lambda is the matrix of eigenvalues Right So its just a and b Those are the diagonal entries of my lambda matrix And then we just find S inverse So we just take negative signs here and recall that we have to divide by the determinant And the determinant of this matrix is just 1 So we just change the signs there Good So this is our nice decomposition of C Now how do we take powers of C Well C to the k is just S lambda to the k S inverse 1 1 2 1 a to the k b to the k 1 1 2 1 Good And multiplying these matrices together just do a little arithmetic here Got a bunch of powers of a and b Because we take powers of the eigenvalues We have here we have 2 b to the k minus a to the k Have a to the k minus b to the k 2 b to the k minus 2 a to the k And finally we get 2 a to the k minus b to the k And this is our kth power matrix Good A quick check Its always good to check your work here Lets plug in k equals 1 And what do we get We get 2b minus a a minus b 2b minus 2a and 2a minus b And if we can go all the way back to our matrix at the very beginning all the way back here that agrees perfectly with what we started with So thats good That means that we did this decomposition right Good So now weve computed the kth power of this matrix Lets do a particular example So lets plug in a and b are 1 So a equals b equals 1 And k equals 100 Then what do we get Well 1 to the 100th is just 1 So were just plugging in 1 for b to the k and a to the k everywhere And we just get in this case C to the 100th is just 1 0 0 1 Its just the identity matrix Great Great OK Now to summarize how do we take powers of a matrix Well first we diagonalize We write our matrix as S lambda S inverse And then we just take powers of the diagonal matrixCS\"\n",
    "encoding_name = \"cl100k_base\"\n",
    "num_tokens = num_tokens_from_string(text, encoding_name)\n",
    "print(f\"Number of tokens in the text: {num_tokens}\")\n",
    "print(len(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
